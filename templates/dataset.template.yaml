apiVersion: batch/v1
kind: Job
metadata:
  name: write-dataset-{{ experiment_name }}
  namespace: kubeflow
spec:
  ttlSecondsAfterFinished: 100
  template:
    metadata:
      name: write-dataset-{{ experiment_name }}
      annotations:
        tf-version.cloud-tpus.google.com: "2.11.0"
    spec:
      containers:
      - name: write-dataset-container
        image: tensorflow/tensorflow:2.11.0
        resources:
          limits:
            cloud-tpus.google.com/preemptible-v2: 8
        command:
        - /bin/sh
        - -c
        - |
          pip install argparse
          cd /tf/notebooks
          python3 dataset.py \
          --gcs_folder={{ gcs_folder }} \
          --gcs_output={{ gcs_output }} \
          --target_size={{ target_size }} \
          --experiment_name={{ experiment_name }} \
          --project_id={{ project_id }} \
          --classes {{ classes }}
        volumeMounts:
        - mountPath: "/tf/notebooks"
          name: ml-service-dataset
        env:
          - name: MLFLOW_TRACKING_URI
            valueFrom:
              secretKeyRef:
                name: mlflow-secret
                key: uri
          - name: MLFLOW_TRACKING_USERNAME
            valueFrom:
              secretKeyRef:
                name: mlflow-secret
                key: username
          - name: MLFLOW_TRACKING_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mlflow-secret
                key: password
          - name: GOOGLE_APPLICATION_CREDENTIALS
            valueFrom:
              secretKeyRef:
                name: mlflow-secret
                key: credential-file
          - name: BACKEND_SERVICE_HOST
            valueFrom:
              secretKeyRef:
                name: app-secret
                key: backend-service
          - name: REDIS_SERVICE_HOST
            value: "redis"
      volumes:
      - name: ml-service-dataset
        persistentVolumeClaim:
          claimName: ml-service-pvc
      restartPolicy: OnFailure